{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70f05a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isdom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\isdom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\isdom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\isdom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tika\n",
    "from tika import parser\n",
    "import re\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93b30982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Договоры для акселератора/Договоры подряда']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "uploaded_file = parser.from_file(\"073a0d372820c3c2bffe9ba24a7ed7af.doc\")\n",
    "#uploaded_file['content']\n",
    "\n",
    "parsed = pd.DataFrame()\n",
    "parsed.loc[0,'content'] = uploaded_file['content']\n",
    "'''\n",
    "bytes_data = uploaded_file.getvalue()\n",
    "data = parser.from_buffer(bytes_data)\n",
    "'''\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('russian')) # множество стоп слов\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer() # для постановки слова в начальную форму\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    '''Функция для лемматизации отдельных слов.'''\n",
    "    final_text = []\n",
    "    for i in text.lower().split():\n",
    "        if i not in stop_words:\n",
    "            parse = morph.parse(i)[0]\n",
    "            if ('Abbr' not in parse.tag):\n",
    "                final_text.append(parse.normal_form)               \n",
    "    return \" \".join(final_text)\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    data = text.copy()\n",
    "\n",
    "    # удаляем пунктуацию\n",
    "    data['content_punct'] = data['content'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Приводим к начальной форме\n",
    "    data['content_punct_lemm'] = data['content_punct'].apply(lemmatize_words)\n",
    "    \n",
    "    return data\n",
    "\n",
    "x_pred=text_preprocessing(parsed)\n",
    "x_pred=x_pred['content_punct_lemm']\n",
    "x_pred= pd.DataFrame(x_pred)\n",
    "\n",
    "import joblib\n",
    "\n",
    "tf = joblib.load('tf.pkl')\n",
    "LR_trained_classifier = joblib.load('LR_trained_classifier.pkl')\n",
    "sample_vec = tf.transform(x_pred)\n",
    "\n",
    "pr = LR_trained_classifier.predict(sample_vec)\n",
    "pr1 = LR_trained_classifier.predict_proba(sample_vec)\n",
    "\n",
    "classes = pd.read_json('classes.json', orient='index')\n",
    "text_class = classes[0].unique()\n",
    "predict = text_class[pr]\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a1b20a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "file must have a 'write' attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1924\\3897438119.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'binary_list.bin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: file must have a 'write' attribute"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "list = [1, 2, 3]\n",
    "pickle.dump(list, 'binary_list.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63640a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Договоры для акселератора/Договоры поставки',\n",
       "       'Договоры для акселератора/Договоры оказания услуг',\n",
       "       'Договоры для акселератора/Договоры подряда',\n",
       "       'Договоры для акселератора/Договоры аренды',\n",
       "       'Договоры для акселератора/Договоры купли-продажи'], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9b8f807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510461e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
